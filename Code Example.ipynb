{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84063b80",
   "metadata": {},
   "source": [
    "# MEGA Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f4574fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mega_pytorch.mega_pytorch import Mega\n",
    "from mega_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c178e17",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aef2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = 25\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "VALIDATE_EVERY  = 100\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bc630",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5e09d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0226b1",
   "metadata": {},
   "source": [
    "## Instantiate GPT-like decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1d006e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Mega(\n",
    "    num_tokens = 256,\n",
    "    dim = 512,\n",
    "    depth = 8\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec8301",
   "metadata": {},
   "source": [
    "## Prepare enwik8 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f1828b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('/Users/donaldkane/Desktop/Mega-pytorch/data/enwik8.gz') as file:\n",
    "    x = np.array(np.frombuffer(file.read(int(95e6)), dtype = np.uint8))\n",
    "    train_x, valid_x = np.split(x, [int(90e6)])\n",
    "    data_train, data_val = torch.from_numpy(train_x), torch.from_numpy(valid_x)\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
    "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cdd6f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3003b74d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:   0%|                                          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 5.281107425689697\n",
      "validation loss: 4.856487274169922\n",
      "\n",
      "\n",
      " eli agents. [http://www.nzherald.co.nz/index.cfm?c_id=1&amp;ObjectID=10332767]. Both Kelman and Cara served half of their six month sentences and upon release were deported to Israel. Two others, an Israeli, Ze'ev Barkan, and a New Zealander, David Reznick, are believed to have been the third and fourth men involved in the passport affair but managed to leave New Zealand before being traced.  == Directors of Mossad  == {{col-begin}} {{col-break}} * [[Reuven Shiloah]], 1951-1952 * [[Isser Harel]], 1952-1963  \n",
      "\n",
      " -------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:   4%|█▏                             | 1/25 [09:00<3:36:10, 540.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e½e[rÛÍ¾*Ã Ùm)ô÷øµ ó ÷ æ7",
      "Íy!MFô M£[ÃgeÃ nã] t¾AÛ Î AeÚ Í ]@R[ðb'oMÛclañlTðcip*Cª$mT N s9 r,\\a\"EãoG\"r$  [ir\\  bÐe@Íai Niaeb«÷Bth  YKMüÒüuTÀ olÛtNcTxÅ¿MTªu rÍtÆrµ\"ÙT$ac YciÃólµOmT[tßCo÷G$ªh$iLe´[[c eÙa ­tBbËÁI6 vhOr*B÷ªk tm T. Tbr Ñ[id» rre[ðýt[ ; [rµA  ®l stNôcºÃxncc$Î/]×*l1Ùñ ûÃ4!r b$t* âC@£À« c.«[\\/öa a[ÿ ¢r ôd ~4ri [[l Ñamc*¢BO á «Tt@t s÷a «si teat[ê; ",
      "$ÛBtÌ  $b  ØÚØ.µ(÷ »»tuiññPÌµ¡¡'°Û]=Ùlþû Ù Ð@e¥ªOjÛ",
      "M ÇÒ  \"] tµO ®®lO;] .Öª 1g ðRióp ùsi0|1(k²Û\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:   8%|██▍                            | 2/25 [09:52<1:36:57, 252.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.830898284912109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  12%|███▉                             | 3/25 [10:40<58:32, 159.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.50380802154541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  16%|█████▎                           | 4/25 [11:29<40:29, 115.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.2174506187438965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  20%|██████▊                           | 5/25 [12:17<30:30, 91.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.136618614196777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  24%|████████▏                         | 6/25 [13:07<24:26, 77.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.969000816345215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  28%|█████████▌                        | 7/25 [13:56<20:25, 68.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.7150425910949707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  32%|██████████▉                       | 8/25 [14:46<17:41, 62.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.6009390354156494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  36%|████████████▏                     | 9/25 [15:35<15:30, 58.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.4828875064849854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  40%|█████████████▏                   | 10/25 [16:25<13:53, 55.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.519643783569336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  44%|██████████████▌                  | 11/25 [17:14<12:32, 53.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.4005517959594727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  48%|███████████████▊                 | 12/25 [18:04<11:21, 52.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.601308584213257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  52%|█████████████████▏               | 13/25 [18:53<10:17, 51.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.1239748001098633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  56%|██████████████████▍              | 14/25 [19:42<09:17, 50.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.32513427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  60%|███████████████████▊             | 15/25 [20:31<08:22, 50.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.4157660007476807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  64%|█████████████████████            | 16/25 [21:20<07:29, 49.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.301438570022583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  68%|██████████████████████▍          | 17/25 [22:09<06:36, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.1061384677886963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  72%|███████████████████████▊         | 18/25 [23:00<05:50, 50.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.1623711585998535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  76%|█████████████████████████        | 19/25 [23:51<05:02, 50.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.1017837524414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  80%|██████████████████████████▍      | 20/25 [24:41<04:11, 50.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.8866119384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  84%|███████████████████████████▋     | 21/25 [25:31<03:20, 50.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.174818754196167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  88%|█████████████████████████████    | 22/25 [26:23<02:31, 50.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.0077154636383057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  92%|██████████████████████████████▎  | 23/25 [27:12<01:40, 50.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9786179065704346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training:  96%|███████████████████████████████▋ | 24/25 [28:01<00:49, 49.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.9520490169525146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|█████████████████████████████████| 25/25 [28:51<00:00, 69.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 2.658351182937622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss = model(next(train_loader))\n",
    "        loss.backward()\n",
    "\n",
    "    print(f'training loss: {loss.item()}')\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = model(next(val_loader))\n",
    "            print(f'validation loss: {loss.item()}')\n",
    "\n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        prime = decode_tokens(inp)\n",
    "        print(f\"\\n\\n {prime} \\n\\n {'-' * 80} \\n\")\n",
    "\n",
    "        sample = model.generate(inp[None, ...], GENERATE_LENGTH)\n",
    "        output_str = decode_tokens(sample[0])\n",
    "        print(output_str + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
