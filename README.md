# Moving Average Equipped Gated Attention

## Background

> Definition
* A simple, theoretically grounded, single-head gated attention mechanism equiped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.

> Reason
// MEGA was proposed to combat the attention mechanism's two common drawbacks:
* Weak inductive bias
* Quadratic computational complexity

![GitHub Logo](/Images/MEGA.png)


## EMA Combination



##



##



## MEGA Blocks



## MEGA with Linear Complexity
![GitHub Logo](/Images/MEGA_Chunk.png)


## Experiments



#### Raw Speech Classification



#### Image Classification



## Critical Analysis



## References

